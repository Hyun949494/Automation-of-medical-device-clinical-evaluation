import requests
import xml.etree.ElementTree as ET
import streamlit as st
import time
import pandas as pd
from config import SEARCH_SETTINGS

def pubmed_search_all(query, email, retmax_per_call=100, api_key=None, mindate=None, maxdate=None):
    """PubMedì—ì„œ ëª¨ë“  ë…¼ë¬¸ ID ìˆ˜ì§‘"""
    base = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi'
    params = {
        'db': 'pubmed',
        'term': query,
        'retmode': 'xml',
        'retmax': retmax_per_call,
        'email': email
    }
    if api_key:
        params['api_key'] = api_key
    
    # ë‚ ì§œ í•„í„°ë¥¼ ì¿¼ë¦¬ì— ì§ì ‘ í¬í•¨
    if mindate and maxdate:
        date_filter = f"{mindate}:{maxdate}[pdat]"
        params['term'] = f"{query} AND {date_filter}"

    all_ids = []
    retstart = 0
    total = None
    
    while True:
        params['retstart'] = retstart
        try:
            r = requests.get(base, params=params, timeout=30)
            
            if r.status_code != 200:
                st.error(f"âŒ HTTP ì˜¤ë¥˜: {r.status_code}")
                break
            
            # XML íŒŒì‹±
            try:
                root = ET.fromstring(r.text)
            except ET.ParseError as e:
                st.error(f"âŒ XML íŒŒì‹± ì‹¤íŒ¨: {e}")
                break
            
            # ì´ ê²°ê³¼ ìˆ˜ í™•ì¸
            if total is None:
                total = int(root.findtext('.//Count', '0'))
                st.success(f"ğŸ“Š **PubMed ì´ ê²°ê³¼**: {total}ê°œ")
                
                if total == 0:
                    st.warning("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ 0ê°œì…ë‹ˆë‹¤")
                    break
                
            # PMID ì¶”ì¶œ
            batch = [e.text for e in root.findall('.//Id')]
            
            if not batch:
                break
                
            all_ids.extend(batch)
            retstart += len(batch)
            
            # ì™„ë£Œ ì¡°ê±´ ì²´í¬
            if retstart >= total or len(all_ids) >= retmax_per_call:
                break
                
        except requests.exceptions.RequestException as e:
            st.error(f"âŒ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
            break
        except Exception as e:
            st.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {type(e).__name__}: {e}")
            break
            
    final_count = len(all_ids)
    st.success(f"ğŸ¯ **ìˆ˜ì§‘ëœ ë…¼ë¬¸**: {final_count}ê°œ")
    
    return all_ids[:retmax_per_call]

def pubmed_details(id_list, email, api_key=None):
    """PMID ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ë…¼ë¬¸ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘"""
    if not id_list:
        return []
        
    base = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi'
    results = []
    chunk = SEARCH_SETTINGS["chunk_size"]
    
    for i in range(0, len(id_list), chunk):
        group = id_list[i:i+chunk]
        params = {
            'db': 'pubmed',
            'id': ','.join(group),
            'retmode': 'xml',
            'email': email
        }
        if api_key:
            params['api_key'] = api_key
            
        try:
            r = requests.get(base, params=params, timeout=60)
            r.raise_for_status()
            root = ET.fromstring(r.text)
            
            for article in root.findall('.//PubmedArticle'):
                pmid = article.findtext('.//PMID', default='')
                
                # ì œëª© ì¶”ì¶œ
                title = article.findtext('.//ArticleTitle', default='')
                if not title:
                    title = f"ì œëª© ì—†ìŒ (PMID: {pmid})"
                
                # Abstract ì¶”ì¶œ
                abstract_parts = []
                for abst in article.findall('.//AbstractText'):
                    if abst.text:
                        label = abst.get('Label', '')
                        text = abst.text
                        if label:
                            abstract_parts.append(f"{label}: {text}")
                        else:
                            abstract_parts.append(text)
                
                abstract = ' '.join(abstract_parts) if abstract_parts else 'ì´ˆë¡ ì—†ìŒ'
                
                # ì €ë„ëª… ì¶”ì¶œ
                journal = article.findtext('.//Journal/Title', default='')
                if not journal:
                    journal = article.findtext('.//MedlineTA', default='ì €ë„ ì—†ìŒ')
                
                # ë°œí–‰ë…„ë„ ì¶”ì¶œ
                year = article.findtext('.//PubDate/Year', default='')
                if not year:
                    year = article.findtext('.//DateCompleted/Year', default='')
                if not year:
                    year = 'ë…„ë„ ì—†ìŒ'
                
                # DOI ì¶”ì¶œ
                doi = ''
                for id_elem in article.findall('.//ArticleId'):
                    if id_elem.attrib.get('IdType') == 'doi':
                        doi = id_elem.text
                        break
                
                # ì €ì ì •ë³´ ì¶”ì¶œ
                authors = []
                for author in article.findall('.//Author')[:5]:
                    lastname = author.findtext('.//LastName', '')
                    forename = author.findtext('.//ForeName', '')
                    if lastname:
                        authors.append(f"{lastname} {forename}".strip())
                
                authors_str = ', '.join(authors) if authors else 'ì €ì ì—†ìŒ'
                if len(article.findall('.//Author')) > 5:
                    authors_str += ' et al.'
                
                results.append({
                    'PMID': pmid,
                    'Title': title[:500] + '...' if len(title) > 500 else title,
                    'Abstract': abstract[:2000] + '...' if len(abstract) > 2000 else abstract,
                    'Authors': authors_str,
                    'Journal': journal,
                    'Year': year,
                    'DOI': doi,
                    'URL': f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/",
                    'Select': '',
                    'Reason': ''
                })
                
        except Exception as e:
            st.error(f"ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            continue
            
        # API ìš”ì²­ ì œí•œ ê³ ë ¤
        time.sleep(SEARCH_SETTINGS["api_delay"])
    
    return results

def build_query(components):
    """PICO êµ¬ì„±ìš”ì†Œë“¤ì„ ANDë¡œ ì—°ê²°í•˜ì—¬ ì¿¼ë¦¬ ìƒì„±"""
    parts = [f"({comp})" for comp in components if comp.strip()]
    return " AND ".join(parts)

def build_filters(filter_options):
    """í•„í„° ì˜µì…˜ë“¤ì„ PubMed ì¿¼ë¦¬ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (PubMed ì›¹ê³¼ ìµœëŒ€í•œ ìœ ì‚¬í•˜ê²Œ)"""
    filters = []

    # Text Availability í•„í„°
    if filter_options.get('text_filters', {}).get('abstract'):
        filters.append('hasabstract[Filter]')
    if filter_options.get('text_filters', {}).get('free_full_text'):
        filters.append('fft[Filter]')
    if filter_options.get('text_filters', {}).get('full_text'):
        filters.append('full text[Filter]')

    # Article Type í•„í„°
    article_type_filters = []
    if filter_options.get('article_type_filters', {}).get('books_docs'):
        article_type_filters.append('booksdocs[Filter]')
    if filter_options.get('article_type_filters', {}).get('clinical_trial'):
        article_type_filters.append('clinicaltrial[Filter]')
    if filter_options.get('article_type_filters', {}).get('meta_analysis'):
        article_type_filters.append('meta-analysis[Filter]')
    if filter_options.get('article_type_filters', {}).get('rct'):
        article_type_filters.append('randomizedcontrolledtrial[Filter]')
    if filter_options.get('article_type_filters', {}).get('review'):
        article_type_filters.append('review[Filter]')
    if filter_options.get('article_type_filters', {}).get('systematic_review'):
        article_type_filters.append('systematicreview[Filter]')

    if article_type_filters:
        filters.append(f"({' OR '.join(article_type_filters)})")

    # Article Attribute í•„í„°
    if filter_options.get('associated_data'):
        filters.append('data[Filter]')

    # Additional í•„í„°
    if filter_options.get('additional_filters', {}).get('human'):
        filters.append('humans[Filter]')
    if filter_options.get('additional_filters', {}).get('english'):
        filters.append('english[Filter]')

    # Species
    if filter_options.get('species', {}).get('humans'):
        filters.append('humans[Filter]')
    if filter_options.get('species', {}).get('other_animals'):
        filters.append('animals[Filter]')

    # Sex
    if filter_options.get('sex', {}).get('female'):
        filters.append('female[Filter]')
    if filter_options.get('sex', {}).get('male'):
        filters.append('male[Filter]')

    # Age
    if filter_options.get('age', {}).get('child'):
        filters.append('child[Filter]')
    if filter_options.get('age', {}).get('adult'):
        filters.append('adult[Filter]')
    if filter_options.get('age', {}).get('aged'):
        filters.append('aged[Filter]')

    # Other
    if filter_options.get('other', {}).get('exclude_preprints'):
        filters.append('NOT preprints[Filter]')
    if filter_options.get('other', {}).get('medline'):
        filters.append('medline[Filter]')        

    return filters
